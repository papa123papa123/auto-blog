#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SERP APIã‚’ä½¿ç”¨ã—ãŸGoogleã‚µã‚¸ã‚§ã‚¹ãƒˆåé›† - è¶…æœ€é©åŒ–ç‰ˆ
APIå‘¼ã³å‡ºã—ã‚’3-4å›ã«æŠ‘ãˆã¦100ä»¶ã®ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚’å–å¾—
"""

import asyncio
import httpx
import json
import os
from datetime import datetime
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
from pathlib import Path
load_dotenv(dotenv_path=Path(__file__).parent / ".env")

class SERPUltraOptimizedCollector:
    def __init__(self):
        self.serp_api_key = os.getenv('SERPAPI_API_KEY')
        if not self.serp_api_key:
            raise ValueError("SERPAPI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.base_url = "https://serpapi.com/search"
        self.collected_keywords = set()
        
    async def get_comprehensive_serp_data(self, keyword: str, start_position: int = 0) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªSERPãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆ1å›ã®APIå‘¼ã³å‡ºã—ã§æœ€å¤§é™ã®æƒ…å ±ã‚’å–å¾—ï¼‰"""
        print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã‹ã‚‰åŒ…æ‹¬çš„SERPãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... (é–‹å§‹ä½ç½®: {start_position})")
        
        params = {
            'q': keyword,
            'api_key': self.serp_api_key,
            'engine': 'google',
            'gl': 'jp',  # æ—¥æœ¬
            'hl': 'ja',  # æ—¥æœ¬èª
            'num': 100,  # æœ€å¤§çµæœæ•°
            'start': start_position,
            'safe': 'active'
        }
        
        try:
            async with httpx.AsyncClient(timeout=90) as client:
                response = await client.get(self.base_url, params=params)
                response.raise_for_status()
                data = response.json()
                
                suggestions = []
                
                # 1. æ¤œç´¢çµæœã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
                if 'organic_results' in data:
                    for result in data['organic_results']:
                        title = result.get('title', '')
                        snippet = result.get('snippet', '')
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                        title_keywords = self.extract_keywords_from_text(title, keyword)
                        suggestions.extend(title_keywords)
                        
                        # ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‹ã‚‰ã‚‚é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                        snippet_keywords = self.extract_keywords_from_text(snippet, keyword)
                        suggestions.extend(snippet_keywords)
                
                # 2. Related Searchesï¼ˆé–¢é€£æ¤œç´¢ï¼‰
                if 'related_searches' in data:
                    for rel_search in data['related_searches']:
                        query = rel_search.get('query', '')
                        if query and len(query) >= 3:
                            suggestions.append(query)
                
                # 3. People Also Askï¼ˆã‚ˆãã‚ã‚‹è³ªå•ï¼‰
                if 'related_questions' in data:
                    for question in data['related_questions']:
                        question_text = question.get('question', '')
                        if question_text and len(question_text) >= 5:
                            suggestions.append(question_text)
                
                # 4. æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã‚µã‚¸ã‚§ã‚¹ãƒˆ
                if 'search_information' in data:
                    search_info = data['search_information']
                    if 'suggested_searches' in search_info:
                        for suggestion in search_info['suggested_searches']:
                            if suggestion and len(suggestion) >= 3:
                                suggestions.append(suggestion)
                
                # 5. ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆã‹ã‚‰ã‚‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
                if 'breadcrumbs' in data:
                    for breadcrumb in data['breadcrumbs']:
                        breadcrumb_text = breadcrumb.get('text', '')
                        if breadcrumb_text and len(breadcrumb_text) >= 3:
                            suggestions.append(breadcrumb_text)
                
                # é‡è¤‡é™¤å»
                unique_suggestions = list(set(suggestions))
                print(f"âœ… åŒ…æ‹¬çš„SERPå–å¾—å®Œäº†: {len(unique_suggestions)}ä»¶")
                
                return {
                    'keyword': keyword,
                    'suggestions': unique_suggestions,
                    'serp_data': data,
                    'start_position': start_position
                }
                
        except Exception as e:
            print(f"âŒ SERPå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {'keyword': keyword, 'suggestions': [], 'serp_data': {}, 'start_position': start_position}
    
    def extract_keywords_from_text(self, text: str, main_keyword: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = []
        
        if not text or len(text) < 3:
            return keywords
        
        # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹èªå¥ã‚’æŠ½å‡º
        related_terms = [
            'ãŠã™ã™ã‚', 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'æ¯”è¼ƒ', 'é¸ã³æ–¹', 'é£²ã¿æ–¹', 'ä½œã‚Šæ–¹', 'ãƒ¬ã‚·ãƒ”',
            'å¤', 'å†¬', 'æ˜¥', 'ç§‹', 'æš‘ã„', 'å¯’ã„', 'å†·ãŸã„', 'ã²ã‚„ãŠã‚ã—',
            'æ—¥æœ¬é…’', 'ãƒ¯ã‚¤ãƒ³', 'ãƒ“ãƒ¼ãƒ«', 'ç„¼é…', 'æ¢…é…’', 'ãƒªã‚­ãƒ¥ãƒ¼ãƒ«',
            'ã‚«ã‚¯ãƒ†ãƒ«', 'ã‚¸ãƒ³', 'ã‚¦ã‚©ãƒƒã‚«', 'ãƒ†ã‚­ãƒ¼ãƒ©', 'ãƒ©ãƒ ', 'ãƒ–ãƒ©ãƒ³ãƒ‡ãƒ¼',
            'åˆå¿ƒè€…', 'å¥³æ€§', 'ç”·æ€§', 'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ', 'ãŠã¤ã¾ã¿', 'å®…é£²ã¿',
            'ã‚³ãƒ³ãƒ“ãƒ‹', 'ã‚¹ãƒ¼ãƒ‘ãƒ¼', 'å±…é…’å±‹', 'ãƒãƒ¼', 'ãƒ‘ãƒ–', 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³',
            'åŠ¹æœ', 'åŠ¹èƒ½', 'ã‚«ãƒ­ãƒªãƒ¼', 'ç³–è³ª', 'ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«åº¦æ•°', 'è³å‘³æœŸé™'
        ]
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é–¢é€£èªå¥ã‚’å«ã‚€ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’æŠ½å‡º
        for term in related_terms:
            if term in text:
                # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è©²å½“éƒ¨åˆ†ã‚’æŠ½å‡º
                start_idx = text.find(term)
                if start_idx >= 0:
                    # å‰å¾Œã®æ–‡è„ˆã‚’å«ã‚ã¦æŠ½å‡ºï¼ˆã‚ˆã‚Šé•·ã„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å–å¾—ï¼‰
                    context_start = max(0, start_idx - 15)
                    context_end = min(len(text), start_idx + len(term) + 15)
                    extracted = text[context_start:context_end].strip()
                    
                    # é©åˆ‡ãªé•·ã•ã®ãƒ•ãƒ¬ãƒ¼ã‚ºã®ã¿ã‚’æ¡ç”¨
                    if 5 <= len(extracted) <= 50:
                        # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
                        cleaned = extracted.replace('\n', ' ').replace('\r', ' ').strip()
                        if cleaned and len(cleaned) >= 5:
                            keywords.append(cleaned)
        
        return keywords
    
    async def collect_suggestions_ultra_optimized(self, strategy_keywords: List[str]) -> Dict[str, Any]:
        """è¶…æœ€é©åŒ–ã•ã‚ŒãŸæ–¹æ³•ã§ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†ã‚’å®Ÿè¡Œ"""
        print(f"ğŸš€ è¶…æœ€é©åŒ–æˆ¦ç•¥ã§ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†é–‹å§‹")
        print("=" * 60)
        
        all_suggestions = []
        serp_results = []
        
        # æˆ¦ç•¥1: æ±ç”¨çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§åŒ…æ‹¬çš„å–å¾—
        print("\nğŸ“Š æˆ¦ç•¥1: æ±ç”¨çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§åŒ…æ‹¬çš„å–å¾—")
        for keyword in strategy_keywords[:2]:  # æœ€åˆã®2ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            # è¤‡æ•°ã®é–‹å§‹ä½ç½®ã‹ã‚‰å–å¾—ï¼ˆã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—ï¼‰
            for start_pos in [0, 10, 20]:  # 3ã¤ã®é–‹å§‹ä½ç½®
                result = await self.get_comprehensive_serp_data(keyword, start_pos)
                if result['suggestions']:
                    all_suggestions.extend(result['suggestions'])
                    serp_results.append(result)
                
                # APIåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å¾…æ©Ÿ
                await asyncio.sleep(2)
        
        # æˆ¦ç•¥2: å­£ç¯€æ€§ãƒ»ç”¨é€”åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è£œå®Œ
        print("\nğŸ“Š æˆ¦ç•¥2: å­£ç¯€æ€§ãƒ»ç”¨é€”åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è£œå®Œ")
        for keyword in strategy_keywords[2:]:  # æ®‹ã‚Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            result = await self.get_comprehensive_serp_data(keyword, 0)
            if result['suggestions']:
                all_suggestions.extend(result['suggestions'])
                serp_results.append(result)
            
            # APIåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å¾…æ©Ÿ
            await asyncio.sleep(2)
        
        # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        unique_suggestions = list(set(all_suggestions))
        
        # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆçŸ­ã™ãã‚‹ã€é•·ã™ãã‚‹ã‚‚ã®ã‚’é™¤å»ï¼‰
        filtered_suggestions = []
        for suggestion in unique_suggestions:
            if 5 <= len(suggestion) <= 50:  # é©åˆ‡ãªé•·ã•
                if not suggestion.startswith('http'):  # URLã§ãªã„
                    if not suggestion.isdigit():  # æ•°å­—ã®ã¿ã§ãªã„
                        filtered_suggestions.append(suggestion)
        
        # çµæœã®ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"serp_ultra_optimized_{len(filtered_suggestions)}ä»¶.json"
        
        result_data = {
            "collection_method": "SERP APIè¶…æœ€é©åŒ–ç‰ˆ",
            "timestamp": timestamp,
            "api_calls": len(strategy_keywords) * 3,  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•° Ã— å¹³å‡3å›
            "strategy_keywords": strategy_keywords,
            "results": {
                "total_unique": len(filtered_suggestions),
                "suggestions": filtered_suggestions,
                "serp_results": serp_results
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä½œæˆ
        txt_filename = f"serp_ultra_optimized_{len(filtered_suggestions)}ä»¶.txt"
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write(f"=== SERP APIè¶…æœ€é©åŒ–ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†çµæœ ===\n")
            f.write(f"åé›†æ–¹æ³•: SERP APIè¶…æœ€é©åŒ–ç‰ˆ\n")
            f.write(f"APIå‘¼ã³å‡ºã—å›æ•°: {result_data['api_calls']}å›\n")
            f.write(f"åé›†æ—¥æ™‚: {timestamp}\n")
            f.write(f"\n")
            f.write(f"=== åé›†çµæœ ===\n")
            f.write(f"ç·ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {len(filtered_suggestions)}ä»¶\n")
            f.write(f"\n")
            f.write(f"ã€æˆ¦ç•¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘\n")
            for i, keyword in enumerate(strategy_keywords, 1):
                f.write(f"{i}. {keyword}\n")
            f.write(f"\n")
            f.write(f"ã€å…¨ã‚µã‚¸ã‚§ã‚¹ãƒˆä¸€è¦§ã€‘\n")
            for i, suggestion in enumerate(filtered_suggestions, 1):
                f.write(f"{i:3d}. {suggestion}\n")
        
        print(f"\nğŸ‰ SERP APIè¶…æœ€é©åŒ–ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†å®Œäº†ï¼")
        print(f"ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
        print(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {txt_filename}")
        print(f"ğŸ“Š ç·åé›†æ•°: {len(filtered_suggestions)}ä»¶")
        print(f"ğŸ’° APIå‘¼ã³å‡ºã—å›æ•°: {result_data['api_calls']}å›")
        
        return result_data
    
    async def collect_single_keyword_suggestions(self, main_keyword: str) -> Dict[str, Any]:
        """å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆèªç¾¤ï¼‰ã‹ã‚‰ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†ã‚’å®Ÿè¡Œ"""
        print(f"ğŸš€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{main_keyword}ã€ã‹ã‚‰è¶…æœ€é©åŒ–ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†é–‹å§‹")
        print("=" * 60)
        
        # 1ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆèªç¾¤ï¼‰ã‹ã‚‰åŒ…æ‹¬çš„ãªSERPãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        result = await self.get_comprehensive_serp_data(main_keyword)
        
        all_suggestions = result['suggestions'] if result['suggestions'] else []
        serp_results = [result] if result else []
        
        # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        unique_suggestions = list(set(all_suggestions))
        
        # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆçŸ­ã™ãã‚‹ã€é•·ã™ãã‚‹ã‚‚ã®ã‚’é™¤å»ï¼‰
        filtered_suggestions = []
        for suggestion in unique_suggestions:
            if 5 <= len(suggestion) <= 50:  # é©åˆ‡ãªé•·ã•
                if not suggestion.startswith('http'):  # URLã§ãªã„
                    if not suggestion.isdigit():  # æ•°å­—ã®ã¿ã§ãªã„
                        filtered_suggestions.append(suggestion)
        
        # çµæœã®ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"serp_ultra_optimized_{len(filtered_suggestions)}ä»¶.json"
        
        result_data = {
            "collection_method": "SERP APIè¶…æœ€é©åŒ–ç‰ˆï¼ˆå˜ä¸€èªç¾¤ï¼‰",
            "timestamp": timestamp,
            "api_calls": 1,  # 1å›ã®APIå‘¼ã³å‡ºã—
            "main_keyword": main_keyword,
            "results": {
                "total_unique": len(filtered_suggestions),
                "suggestions": filtered_suggestions,
                "serp_results": serp_results
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä½œæˆ
        txt_filename = f"serp_ultra_optimized_{len(filtered_suggestions)}ä»¶.txt"
        with open(txt_filename, 'w', encoding='utf-8') as f:
            f.write(f"=== SERP APIè¶…æœ€é©åŒ–ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†çµæœ ===\n")
            f.write(f"åé›†æ–¹æ³•: SERP APIè¶…æœ€é©åŒ–ç‰ˆï¼ˆå˜ä¸€èªç¾¤ï¼‰\n")
            f.write(f"APIå‘¼ã³å‡ºã—å›æ•°: 1å›\n")
            f.write(f"åé›†æ—¥æ™‚: {timestamp}\n")
            f.write(f"\n")
            f.write(f"=== åé›†çµæœ ===\n")
            f.write(f"ç·ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {len(filtered_suggestions)}ä»¶\n")
            f.write(f"\n")
            f.write(f"ã€ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‘\n")
            f.write(f"{main_keyword}\n")
            f.write(f"\n")
            f.write(f"ã€å…¨ã‚µã‚¸ã‚§ã‚¹ãƒˆä¸€è¦§ã€‘\n")
            for i, suggestion in enumerate(filtered_suggestions, 1):
                f.write(f"{i:3d}. {suggestion}\n")
        
        print(f"\nSERP APIè¶…æœ€é©åŒ–ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†å®Œäº†ï¼")
        print(f"ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
        print(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {txt_filename}")
        print(f"ğŸ“Š ç·åé›†æ•°: {len(filtered_suggestions)}ä»¶")
        print(f"ğŸ’° APIå‘¼ã³å‡ºã—å›æ•°: 1å›")
        
        return result_data

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        print("SERP APIè¶…æœ€é©åŒ–ã‚µã‚¸ã‚§ã‚¹ãƒˆåé›†ãƒ„ãƒ¼ãƒ«")
        print("=" * 50)
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ã‚’å–å¾—
        print("\nğŸ“ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§è¤‡æ•°å…¥åŠ›å¯èƒ½ï¼‰:")
        print("ä¾‹: é…’ ãŠã™ã™ã‚ å¤")
        print("ä¾‹: ãŠé…’ åˆå¿ƒè€… é¸ã³æ–¹")
        print("ä¾‹: å¤ ãŠé…’ æš‘ã„ ã‚«ã‚¯ãƒ†ãƒ«")
        
        user_input = input("\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ").strip()
        
        if not user_input:
            print("âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        # å…¥åŠ›ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1ã¤ã®èªç¾¤ã¨ã—ã¦å‡¦ç†
        main_keyword = user_input.strip()
        
        print(f"\nğŸ¯ å…¥åŠ›ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {main_keyword}")
        print(f"ğŸ“Š å‡¦ç†æ–¹æ³•: 1ã¤ã®èªç¾¤ã¨ã—ã¦SERP APIå‘¼ã³å‡ºã—")
        print(f"ğŸ’° äºˆæƒ³APIå‘¼ã³å‡ºã—å›æ•°: 1å›")
        
        # å®Ÿè¡Œç¢ºèª
        confirm = input("\nå®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()
        if confirm not in ['y', 'yes', 'ã¯ã„']:
            print("âŒ å®Ÿè¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            return
        
        print("\n" + "=" * 60)
        
        collector = SERPUltraOptimizedCollector()
        result = await collector.collect_single_keyword_suggestions(main_keyword)
        
        if result['results']['total_unique'] >= 100:
            print(f"\nç›®æ¨™é”æˆï¼100ä»¶ä»¥ä¸Šã®ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ")
            print(f"ğŸ¯ åŠ¹ç‡: {result['results']['total_unique']}ä»¶ Ã· {result['api_calls']}å› = {result['results']['total_unique']/result['api_calls']:.1f}ä»¶/å›")
        else:
            print(f"\nç›®æ¨™æœªé”æˆã€‚ç¾åœ¨{result['results']['total_unique']}ä»¶ã§ã™")
            print("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’èª¿æ•´ã™ã‚‹ã‹ã€ã‚ˆã‚Šå…·ä½“çš„ãªèªç¾¤ã‚’è©¦ã—ã¦ãã ã•ã„")
        
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
