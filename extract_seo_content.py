#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SEOç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å…¥åŠ›: serp_collected_[ä»¶æ•°]ä»¶.json
å‡ºåŠ›: seo_content_[ä»¶æ•°]ä»¶.txt
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any
import re

def extract_seo_content(serp_data: dict) -> List[str]:
    """SERPãƒ‡ãƒ¼ã‚¿ã‹ã‚‰SEOç”¨ã®å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
    print("ğŸ“ SERPãƒ‡ãƒ¼ã‚¿ã‹ã‚‰SEOç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºä¸­...")
    all_content = []
    
    # 1. æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ã‚µã‚¸ã‚§ã‚¹ãƒˆï¼ˆresults.suggestionsï¼‰
    if 'results' in serp_data and 'suggestions' in serp_data['results']:
        suggestions = serp_data['results']['suggestions']
        all_content.extend(suggestions)
        print(f"  ğŸ“Œ åŸºæœ¬ã‚µã‚¸ã‚§ã‚¹ãƒˆ: {len(suggestions)}ä»¶")
    
    # 2. SERPãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥æŠ½å‡º
    if 'results' in serp_data and 'serp_results' in serp_data['results']:
        for serp_result in serp_data['results']['serp_results']:
            if 'serp_data' in serp_result:
                data = serp_result['serp_data']
                
                # é–¢é€£æ¤œç´¢
                if 'related_searches' in data:
                    for search in data['related_searches']:
                        if isinstance(search, dict) and 'query' in search:
                            all_content.append(search['query'])
                        elif isinstance(search, str):
                            all_content.append(search)
                    print(f"  ğŸ“Œ é–¢é€£æ¤œç´¢: {len(data['related_searches'])}ä»¶")
                
                # é–¢é€£è³ªå•
                if 'related_questions' in data:
                    for question in data['related_questions']:
                        if isinstance(question, dict):
                            if 'question' in question:
                                all_content.append(question['question'])
                            if 'snippet' in question:
                                all_content.append(question['snippet'])
                            
                            # ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ã‚‚æƒ…å ±ã‚’æŠ½å‡º
                            if 'text_blocks' in question:
                                for block in question['text_blocks']:
                                    if isinstance(block, dict):
                                        if 'snippet' in block:
                                            all_content.append(block['snippet'])
                                        if 'list' in block:
                                            for item in block['list']:
                                                if isinstance(item, dict):
                                                    if 'title' in item:
                                                        all_content.append(item['title'])
                                                    if 'snippet' in item:
                                                        all_content.append(item['snippet'])
                    print(f"  ğŸ“Œ é–¢é€£è³ªå•: {len(data['related_questions'])}ä»¶")
                
                # People Also Ask
                if 'people_also_ask' in data:
                    for paa in data['people_also_ask']:
                        if isinstance(paa, dict):
                            if 'question' in paa:
                                all_content.append(paa['question'])
                            if 'answer' in paa:
                                all_content.append(paa['answer'])
                    print(f"  ğŸ“Œ ã‚ˆãã‚ã‚‹è³ªå•: {len(data['people_also_ask'])}ä»¶")
                
                # AIæ¦‚è¦
                if 'ai_overview' in data:
                    ai_data = data['ai_overview']
                    if isinstance(ai_data, dict):
                        if 'questions' in ai_data:
                            for q in ai_data['questions']:
                                if isinstance(q, dict) and 'question' in q:
                                    all_content.append(q['question'])
                                elif isinstance(q, str):
                                    all_content.append(q)
                    print(f"  ğŸ“Œ AIæ¦‚è¦: 1ä»¶")
                
                # ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒ‹ãƒšãƒƒãƒˆ
                if 'featured_snippet' in data:
                    snippet = data['featured_snippet']
                    if isinstance(snippet, dict):
                        if 'title' in snippet:
                            all_content.append(snippet['title'])
                        if 'snippet' in snippet:
                            all_content.append(snippet['snippet'])
                    print(f"  ğŸ“Œ ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒ‹ãƒšãƒƒãƒˆ: 1ä»¶")
                
                # æ¤œç´¢çµæœã‚¿ã‚¤ãƒˆãƒ«
                if 'organic_results' in data:
                    for result in data['organic_results']:
                        if 'title' in result:
                            all_content.append(result['title'])
                    print(f"  ğŸ“Œ æ¤œç´¢çµæœã‚¿ã‚¤ãƒˆãƒ«: {len(data['organic_results'])}ä»¶")
            
            elif 'suggestions' in serp_result:
                all_content.extend(serp_result['suggestions'])
    
    # é‡è¤‡é™¤å»ã¨ã‚½ãƒ¼ãƒˆ
    unique_content = list(set(all_content))
    unique_content.sort()
    
    print(f"âœ… æŠ½å‡ºå®Œäº†: {len(all_content)}ä»¶ â†’ {len(unique_content)}ä»¶ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
    
    return unique_content

def filter_content(content_list: List[str]) -> List[str]:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦å“è³ªã‚’å‘ä¸Š"""
    print("ğŸ” ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")
    
    filtered_content = []
    removed_count = 0
    
    # ç¾åœ¨ã®æ—¥ä»˜ã‚’å–å¾—
    from datetime import datetime, timedelta
    current_date = datetime.now()
    one_month_ago = current_date - timedelta(days=30)
    
    for content in content_list:
        # URLã‚’é™¤å»
        if content.startswith('http') or '://' in content:
            removed_count += 1
            continue
        
        # ç„¡æ„å‘³ãªæ–‡å­—åˆ—ï¼ˆè‹±æ•°å­—ã®ã¿ã€ã¾ãŸã¯éå¸¸ã«çŸ­ã„ï¼‰ã‚’é™¤å»
        if len(content) < 5 or content.isalnum():
            removed_count += 1
            continue
        
        # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ200æ–‡å­—ä»¥ä¸Šï¼‰ã‚’é™¤å»
        if len(content) > 200:
            removed_count += 1
            continue
        
        # æ—¥ä»˜ã®ã¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
        if re.match(r'^\d{4}[-/]\d{2}[-/]\d{2}', content.strip()):
            removed_count += 1
            continue
        
        # è‹±æ•°å­—ã®ã¿ã®æ–‡å­—åˆ—ã‚’é™¤å»
        if re.match(r'^[a-zA-Z0-9_-]+$', content.strip()):
            removed_count += 1
            continue
        
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’é™¤å»ï¼ˆ.com, .jp, .co.jp ãªã©ï¼‰
        if re.match(r'^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', content.strip()):
            removed_count += 1
            continue
        
        # å¤ã„æ—¥ä»˜ã‚’å«ã‚€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»ï¼ˆæ˜¨å¹´ç‰ˆã€2023å¹´ãªã©ï¼‰
        if re.search(r'202[0-3]å¹´|æ˜¨å¹´ç‰ˆ|å‰å¹´ç‰ˆ|æ—§å¹´ç‰ˆ', content):
            removed_count += 1
            continue
        
        # ã‚ˆã‚Šå…·ä½“çš„ãªå¤ã„æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
        if re.search(r'202[0-3][-/]\d{2}', content):
            removed_count += 1
            continue
        
        filtered_content.append(content)
    
    print(f"âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {len(content_list)}ä»¶ â†’ {len(filtered_content)}ä»¶ï¼ˆ{removed_count}ä»¶é™¤å»ï¼‰")
    return filtered_content

def save_to_text_file(content_list: List[str], output_filename: str, original_filename: str):
    """æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    print(f"ğŸ“„ çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ä¸­: {output_filename}")
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        for i, content in enumerate(content_list, 1):
            f.write(f"{i:3d}. {content}\n")
    
    print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ: {output_filename}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” SEOç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    json_files = []
    
    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
    for file in os.listdir('.'):
        if (file.startswith('serp_collected_') or file.startswith('serp_optimized_collected_')) and file.endswith('.json'):
            json_files.append(file)
    
    # serp_resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚æ¤œç´¢
    serp_results_dir = 'serp_results'
    if os.path.exists(serp_results_dir):
        for file in os.listdir(serp_results_dir):
            if (file.startswith('serp_collected_') or file.startswith('serp_optimized_collected_')) and file.endswith('.json'):
                json_files.append(os.path.join(serp_results_dir, file))
    
    if not json_files:
        print("âŒ serp_*_collected_*.json ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   å…ˆã« collect_google_suggestions.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ä»¶æ•°ã‚’æŠ½å‡ºã—ã¦ã€æ•°å€¤é †ã§ã‚½ãƒ¼ãƒˆ
    def extract_count(filename):
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ä»¶æ•°ã‚’æŠ½å‡º
            # ä¾‹: "serp_optimized_collected_141ä»¶.json" â†’ 141
            # ä¾‹: "serp_collected_76ä»¶.json" â†’ 76
            # ä¾‹: "serp_collected_323ä»¶.json" â†’ 323
            
            # "ä»¶.json" ã®å‰ã®éƒ¨åˆ†ã‚’å–å¾—
            if 'ä»¶.json' in filename:
                parts = filename.split('ä»¶.json')[0].split('_')
                # æœ€å¾Œã®éƒ¨åˆ†ãŒä»¶æ•°
                last_part = parts[-1]
                if last_part.isdigit():
                    return int(last_part)
            
            # åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: "ä»¶" ã®å‰ã®éƒ¨åˆ†
            if 'ä»¶' in filename:
                parts = filename.split('ä»¶')[0].split('_')
                last_part = parts[-1]
                if last_part.isdigit():
                    return int(last_part)
            
            return 0
        except:
            return 0
    
    # ä»¶æ•°é †ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
    json_files.sort(key=extract_count, reverse=True)
    
    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    print("ğŸ” æ¤œç´¢ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
    for i, file in enumerate(json_files[:10]):  # ä¸Šä½10ä»¶ã‚’è¡¨ç¤º
        count = extract_count(file)
        print(f"  {i+1}. {file} (ä»¶æ•°: {count}ä»¶)")
    
    # 141ä»¶ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    target_file = None
    for file in json_files:
        if 'serp_optimized_collected_141ä»¶.json' in file:
            target_file = file
            break
    
    if target_file:
        selected_file = target_file
        print(f"ğŸ¯ æŒ‡å®šã•ã‚ŒãŸ141ä»¶ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ: {selected_file}")
    else:
        selected_file = json_files[0]
        print(f"ğŸ“ è‡ªå‹•é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {selected_file} (ä»¶æ•°: {extract_count(selected_file)}ä»¶)")
    
    try:
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(selected_file, 'r', encoding='utf-8') as f:
            serp_data = json.load(f)
        
        print(f"âœ… JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")
        
        # å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        all_content = extract_seo_content(serp_data)
        
        if not all_content:
            print("âŒ æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_content = filter_content(all_content)
        
        # çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        # ãƒ•ã‚¡ã‚¤ãƒ«åã«ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ä½œæˆæ—¥æ™‚ã‚’è¿½åŠ 
        from datetime import datetime
        current_time = datetime.now()
        time_str = current_time.strftime("%Y%m%d_%H%M")
        
        # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨æ¸¬ï¼‰
        main_keyword = "å¤_ãŠã™ã™ã‚_é…’"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        if 'serp_optimized_collected_141ä»¶.json' in selected_file:
            main_keyword = "å¤_ãŠã™ã™ã‚_é…’"  # 141ä»¶ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir = "seo_results"
        os.makedirs(output_dir, exist_ok=True)
        
        output_filename = f"seo_content_{main_keyword}_{time_str}_{len(filtered_content)}ä»¶.txt"
        output_filepath = os.path.join(output_dir, output_filename)
        save_to_text_file(filtered_content, output_filepath, selected_file)
        
        # çµæœã‚µãƒãƒªãƒ¼
        print(f"\nğŸ‰ æŠ½å‡ºå‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“Š æŠ½å‡ºä»¶æ•°: {len(filtered_content)}ä»¶")
        print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_filepath}")
        
        # æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
        print(f"\nğŸ“ æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆæœ€åˆã®10ä»¶ï¼‰:")
        for i, content in enumerate(filtered_content[:10], 1):
            print(f"{i:2d}. {content}")
        
        if len(filtered_content) > 10:
            print(f"... ä»– {len(filtered_content) - 10}ä»¶")
        
        # çµ±è¨ˆæƒ…å ±
        print(f"\nğŸ“ˆ çµ±è¨ˆæƒ…å ±:")
        print(f"  - å…ƒãƒ•ã‚¡ã‚¤ãƒ«: {selected_file}")
        print(f"  - æŠ½å‡ºä»¶æ•°: {len(filtered_content)}ä»¶")
        print(f"  - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_filepath}")
        
        # ç›®æ¨™ä»¶æ•°ã¨ã®æ¯”è¼ƒ
        if len(filtered_content) >= 200:
            print(f"ğŸ¯ ç›®æ¨™é”æˆï¼ 200ä»¶ä»¥ä¸Šï¼ˆ{len(filtered_content)}ä»¶ï¼‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        else:
            print(f"âš ï¸  ç›®æ¨™æœªé”: {len(filtered_content)}ä»¶ï¼ˆç›®æ¨™: 200ä»¶ä»¥ä¸Šï¼‰")
            
    except FileNotFoundError:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {selected_file}")
    except json.JSONDecodeError:
        print(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: {selected_file}")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
