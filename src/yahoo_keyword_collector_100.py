# src/yahoo_keyword_collector_100.py
# Yahooæ¤œç´¢ãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé«˜é€Ÿ100å€‹ç‰ˆãƒ»SERP APIä¸è¦ï¼‰

import asyncio
import aiohttp
import re
from pathlib import Path
from typing import List, Set, Dict, Optional
from urllib.parse import quote, urlencode
import time
import random
import logging

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class YahooKeywordCollector100:
    """Yahooæ¤œç´¢ã‹ã‚‰é«˜é€Ÿã§100å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str = "yahoo_keywords_100", delay_range: tuple = (0.2, 0.5)):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # é…å»¶è¨­å®šï¼ˆè¶…é«˜é€ŸåŒ–ï¼‰
        self.delay_range = delay_range
        
        # Yahooæ¤œç´¢ã®ãƒ™ãƒ¼ã‚¹URL
        self.base_url = "https://search.yahoo.co.jp/search"
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        ]
        
        print("[OK] YahooKeywordCollector100ã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸã€‚ï¼ˆé«˜é€Ÿ100å€‹ç‰ˆï¼‰")
    
    async def collect_all_keywords(self, main_keyword: str) -> List[str]:
        """ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰é«˜é€Ÿã§100å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†"""
        start_time = time.time()
        print(f"\n=== ã€Œ{main_keyword}ã€ã®é«˜é€Ÿ100å€‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åé›†é–‹å§‹ ===")
        
        all_keywords: Set[str] = set()
        
        # 1. ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åŸºæœ¬æ¤œç´¢ã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†
        print("\n[ã‚¹ãƒ†ãƒƒãƒ—1/4] ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åŸºæœ¬æ¤œç´¢ã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ä¸­...")
        basic_keywords = await self._collect_basic_keywords(main_keyword)
        all_keywords.update(basic_keywords)
        print(f"  -> {len(basic_keywords)}å€‹ã®åŸºæœ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ã—ã¾ã—ãŸã€‚")
        
        # 2. è‡ªç„¶ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åé›†ï¼ˆæˆ¦ç•¥çš„æ‹¡å¼µãƒ¯ãƒ¼ãƒ‰ä¸ä½¿ç”¨ï¼‰
        print("\n[ã‚¹ãƒ†ãƒƒãƒ—2/4] è‡ªç„¶ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ä¸­...")
        natural_keywords = await self._collect_natural_suggestions(main_keyword)
        all_keywords.update(natural_keywords)
        print(f"  -> {len(natural_keywords)}å€‹ã®è‡ªç„¶ã‚µã‚¸ã‚§ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ã—ã¾ã—ãŸã€‚")
        
        # 3. è¤‡æ•°ãƒšãƒ¼ã‚¸ã®æ¤œç´¢çµæœã‚’ä¸¦åˆ—è§£æ
        print("\n[ã‚¹ãƒ†ãƒƒãƒ—3/4] è¤‡æ•°ãƒšãƒ¼ã‚¸ã®æ¤œç´¢çµæœã‚’ä¸¦åˆ—è§£æä¸­...")
        multi_page_keywords = await self._collect_multi_page_keywords(main_keyword)
        all_keywords.update(multi_page_keywords)
        print(f"  -> {len(multi_page_keywords)}å€‹ã®è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ã—ã¾ã—ãŸã€‚")
        
        # 4. é–¢é€£æ¤œç´¢ã®æ·±æ˜ã‚Šï¼ˆå¤§å¹…æ‹¡å¼µãƒ»ä¸¦åˆ—å®Ÿè¡Œï¼‰
        print("\n[ã‚¹ãƒ†ãƒƒãƒ—4/4] é–¢é€£æ¤œç´¢ã®æ·±æ˜ã‚Šã‚’å¤§å¹…æ‹¡å¼µãƒ»ä¸¦åˆ—å®Ÿè¡Œä¸­...")
        deep_keywords = await self._collect_deep_keywords_extended(main_keyword, list(all_keywords)[:15])
        all_keywords.update(deep_keywords)
        print(f"  -> {len(deep_keywords)}å€‹ã®æ·±æ˜ã‚Šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ã—ã¾ã—ãŸã€‚")
        
        # çµæœã‚’æ•´ç†
        final_keywords = sorted(list(all_keywords))
        elapsed_time = time.time() - start_time
        
        print(f"\nâœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åé›†å®Œäº†ï¼ åˆè¨ˆ {len(final_keywords)}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ã—ã¾ã—ãŸã€‚")
        print(f"â±ï¸  å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’")
        
        # 100å€‹ç›®æ¨™ã®é”æˆçŠ¶æ³
        if len(final_keywords) >= 100:
            print(f"ğŸ¯ ç›®æ¨™é”æˆï¼ 100å€‹ä»¥ä¸Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ã—ã¾ã—ãŸã€‚")
        else:
            print(f"ğŸ“Š ç›®æ¨™ã¾ã§ã‚ã¨ {100 - len(final_keywords)}å€‹")
        
        return final_keywords
    
    async def _collect_basic_keywords(self, main_keyword: str) -> List[str]:
        """ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åŸºæœ¬æ¤œç´¢ã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†"""
        keywords = set()
        
        # åŸºæœ¬æ¤œç´¢ã‚’å®Ÿè¡Œ
        html_content = await self._fetch_yahoo_search(main_keyword)
        if html_content:
            # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            related_keywords = self._extract_related_keywords(html_content)
            keywords.update(related_keywords)
            
            # æ¤œç´¢çµæœã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            title_keywords = self._extract_title_keywords(html_content)
            keywords.update(title_keywords)
            
            # æ¤œç´¢çµæœã®èª¬æ˜æ–‡ã‹ã‚‰ã‚‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            description_keywords = self._extract_description_keywords(html_content)
            keywords.update(description_keywords)
            
            # æ¤œç´¢çµæœã®URLã‹ã‚‰ã‚‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            url_keywords = self._extract_url_keywords(html_content)
            keywords.update(url_keywords)
        
        return list(keywords)
    
    async def _collect_natural_suggestions(self, main_keyword: str) -> List[str]:
        """è‡ªç„¶ãªã‚µã‚¸ã‚§ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ï¼ˆæˆ¦ç•¥çš„æ‹¡å¼µãƒ¯ãƒ¼ãƒ‰ä¸ä½¿ç”¨ï¼‰"""
        keywords = set()
        
        # ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œç´¢çµæœã‹ã‚‰è‡ªç„¶ã«å‡ºã¦ãã‚‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        html_content = await self._fetch_yahoo_search(main_keyword)
        if html_content:
            # ã‚ˆã‚Šè©³ç´°ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            natural_keywords = self._extract_natural_suggestions(html_content)
            keywords.update(natural_keywords)
            
            # æ¤œç´¢çµæœã®ä¸‹éƒ¨ã«è¡¨ç¤ºã•ã‚Œã‚‹ã€Œé–¢é€£ã™ã‚‹æ¤œç´¢ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            bottom_suggestions = self._extract_bottom_suggestions(html_content)
            keywords.update(bottom_suggestions)
            
            # æ¤œç´¢çµæœã®å³å´ã«è¡¨ç¤ºã•ã‚Œã‚‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            right_suggestions = self._extract_right_suggestions(html_content)
            keywords.update(right_suggestions)
            
            # æ¤œç´¢çµæœã®ä¸Šéƒ¨ã«è¡¨ç¤ºã•ã‚Œã‚‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            top_suggestions = self._extract_top_suggestions(html_content)
            keywords.update(top_suggestions)
        
        return list(keywords)
    
    async def _collect_multi_page_keywords(self, main_keyword: str) -> List[str]:
        """è¤‡æ•°ãƒšãƒ¼ã‚¸ã®æ¤œç´¢çµæœã‚’ä¸¦åˆ—è§£æã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†"""
        keywords = set()
        
        # 1-3ãƒšãƒ¼ã‚¸ç›®ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        tasks = []
        for page in range(1, 4):
            task = self._fetch_and_extract_page_keywords(main_keyword, page)
            tasks.append(task)
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœã‚’çµ±åˆ
        for result in results:
            if isinstance(result, list):
                keywords.update(result)
            else:
                print(f"  -> [WARN] è¤‡æ•°ãƒšãƒ¼ã‚¸è§£æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {result}")
        
        return list(keywords)
    
    async def _fetch_and_extract_page_keywords(self, main_keyword: str, page: int) -> List[str]:
        """æŒ‡å®šãƒšãƒ¼ã‚¸ã®æ¤œç´¢çµæœã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        # ãƒšãƒ¼ã‚¸æŒ‡å®šã®æ¤œç´¢ã‚’å®Ÿè¡Œ
        html_content = await self._fetch_yahoo_search_page(main_keyword, page)
        if html_content:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            title_keywords = self._extract_title_keywords(html_content)
            keywords.update(title_keywords)
            
            # èª¬æ˜æ–‡ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            description_keywords = self._extract_description_keywords(html_content)
            keywords.update(description_keywords)
            
            # URLã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            url_keywords = self._extract_url_keywords(html_content)
            keywords.update(url_keywords)
        
        return list(keywords)
    
    async def _collect_deep_keywords_extended(self, main_keyword: str, seed_keywords: List[str]) -> List[str]:
        """åé›†ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ·±æ˜ã‚Šï¼ˆå¤§å¹…æ‹¡å¼µãƒ»ä¸¦åˆ—å®Ÿè¡Œï¼‰"""
        keywords = set()
        
        # ä¸Šä½15å€‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ·±æ˜ã‚Šï¼ˆ8å€‹ã‹ã‚‰æ‹¡å¼µï¼‰
        tasks = []
        for seed_keyword in seed_keywords[:15]:
            task = self._fetch_and_extract_deep_keywords(seed_keyword)
            tasks.append(task)
        
        # ä¸¦åˆ—å®Ÿè¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœã‚’çµ±åˆ
        for result in results:
            if isinstance(result, list):
                keywords.update(result)
            else:
                print(f"  -> [WARN] æ·±æ˜ã‚Šã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {result}")
        
        return list(keywords)
    
    async def _fetch_and_extract_deep_keywords(self, seed_keyword: str) -> List[str]:
        """ã‚·ãƒ¼ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ·±æ˜ã‚Šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—"""
        html_content = await self._fetch_yahoo_search(seed_keyword)
        if html_content:
            return self._extract_related_keywords(html_content)
        return []
    
    async def _fetch_yahoo_search(self, query: str) -> Optional[str]:
        """Yahooæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦HTMLã‚’å–å¾—"""
        try:
            # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠ
            user_agent = random.choice(self.user_agents)
            
            # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            params = {
                'p': query,
                'ei': 'UTF-8',
                'fr': 'top_ga1_sa'
            }
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            headers = {
                'User-Agent': user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            async with aiohttp.ClientSession() as session:
                url = f"{self.base_url}?{urlencode(params)}"
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # HTMLã‚’ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                        safe_filename = self._make_safe_filename(query)
                        file_path = self.output_dir / f"{safe_filename}.html"
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(content)
                        
                        return content
                    else:
                        print(f"  -> [WARN] æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã§HTTP {response.status}ãŒè¿”ã•ã‚Œã¾ã—ãŸã€‚")
                        return None
                        
        except Exception as e:
            print(f"  -> [ERROR] æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            return None
    
    async def _fetch_yahoo_search_page(self, query: str, page: int) -> Optional[str]:
        """æŒ‡å®šãƒšãƒ¼ã‚¸ã®Yahooæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦HTMLã‚’å–å¾—"""
        try:
            # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠ
            user_agent = random.choice(self.user_agents)
            
            # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒšãƒ¼ã‚¸æŒ‡å®šï¼‰
            params = {
                'p': query,
                'ei': 'UTF-8',
                'fr': 'top_ga1_sa',
                'b': (page - 1) * 10 + 1  # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’è¨ˆç®—
            }
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            headers = {
                'User-Agent': user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            async with aiohttp.ClientSession() as session:
                url = f"{self.base_url}?{urlencode(params)}"
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # HTMLã‚’ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                        safe_filename = self._make_safe_filename(f"{query}_page{page}")
                        file_path = self.output_dir / f"{safe_filename}.html"
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(content)
                        
                        return content
                    else:
                        print(f"  -> [WARN] ãƒšãƒ¼ã‚¸{page}ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã§HTTP {response.status}ãŒè¿”ã•ã‚Œã¾ã—ãŸã€‚")
                        return None
                        
        except Exception as e:
            print(f"  -> [ERROR] ãƒšãƒ¼ã‚¸{page}ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã€Œ{query}ã€ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            return None
    
    def _extract_related_keywords(self, html_content: str) -> List[str]:
        """HTMLã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆåŸºæœ¬ç‰ˆï¼‰"""
        keywords = set()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã€Œé–¢é€£ã™ã‚‹æ¤œç´¢ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        related_patterns = [
            r'<a[^>]*class="[^"]*related[^"]*"[^>]*>([^<]+)</a>',
            r'<span[^>]*class="[^"]*related[^"]*"[^>]*>([^<]+)</span>',
            r'é–¢é€£ã™ã‚‹æ¤œç´¢[^>]*>([^<]+)</a>',
            r'é–¢é€£æ¤œç´¢[^>]*>([^<]+)</a>'
        ]
        
        for pattern in related_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                clean_text = re.sub(r'<[^>]+>', '', match).strip()
                if clean_text and len(clean_text) > 2:
                    keywords.add(clean_text)
        
        return list(keywords)
    
    def _extract_natural_suggestions(self, html_content: str) -> List[str]:
        """è‡ªç„¶ãªã‚µã‚¸ã‚§ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        # æ¤œç´¢çµæœã®èª¬æ˜æ–‡ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        description_patterns = [
            r'<p[^>]*class="[^"]*description[^"]*"[^>]*>([^<]+)</p>',
            r'<div[^>]*class="[^"]*description[^"]*"[^>]*>([^<]+)</div>',
            r'<span[^>]*class="[^"]*description[^"]*"[^>]*>([^<]+)</span>'
        ]
        
        for pattern in description_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                clean_text = re.sub(r'<[^>]+>', '', match).strip()
                if clean_text:
                    # èª¬æ˜æ–‡ã‹ã‚‰é‡è¦ãªå˜èªã‚’æŠ½å‡º
                    words = re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+', clean_text)
                    for word in words:
                        if len(word) > 1 and len(word) < 15:  # é©åˆ‡ãªé•·ã•ã®å˜èªã®ã¿
                            keywords.add(word)
        
        return list(keywords)
    
    def _extract_bottom_suggestions(self, html_content: str) -> List[str]:
        """æ¤œç´¢çµæœã®ä¸‹éƒ¨ã«è¡¨ç¤ºã•ã‚Œã‚‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        # æ¤œç´¢çµæœã®ä¸‹éƒ¨ã«è¡¨ç¤ºã•ã‚Œã‚‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        bottom_patterns = [
            r'<div[^>]*class="[^"]*bottom[^"]*"[^>]*>([^<]+)</div>',
            r'<ul[^>]*class="[^"]*related[^"]*"[^>]*>(.*?)</ul>',
            r'<li[^>]*class="[^"]*related[^"]*"[^>]*>([^<]+)</li>',
            r'<div[^>]*class="[^"]*suggestion[^"]*"[^>]*>([^<]+)</div>'
        ]
        
        for pattern in bottom_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                clean_text = re.sub(r'<[^>]+>', '', match).strip()
                if clean_text and len(clean_text) > 2:
                    keywords.add(clean_text)
        
        return list(keywords)
    
    def _extract_right_suggestions(self, html_content: str) -> List[str]:
        """æ¤œç´¢çµæœã®å³å´ã«è¡¨ç¤ºã•ã‚Œã‚‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        # å³å´ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¡¨ç¤ºã•ã‚Œã‚‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        right_patterns = [
            r'<div[^>]*class="[^"]*sidebar[^"]*"[^>]*>(.*?)</div>',
            r'<div[^>]*class="[^"]*right[^"]*"[^>]*>(.*?)</div>',
            r'<aside[^>]*>(.*?)</aside>'
        ]
        
        for pattern in right_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                link_matches = re.findall(r'<a[^>]*>([^<]+)</a>', match)
                for link_text in link_matches:
                    clean_text = re.sub(r'<[^>]+>', '', link_text).strip()
                    if clean_text and len(clean_text) > 2:
                        keywords.add(clean_text)
        
        return list(keywords)
    
    def _extract_top_suggestions(self, html_content: str) -> List[str]:
        """æ¤œç´¢çµæœã®ä¸Šéƒ¨ã«è¡¨ç¤ºã•ã‚Œã‚‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        # æ¤œç´¢çµæœã®ä¸Šéƒ¨ã«è¡¨ç¤ºã•ã‚Œã‚‹é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        top_patterns = [
            r'<div[^>]*class="[^"]*top[^"]*"[^>]*>([^<]+)</div>',
            r'<div[^>]*class="[^"]*header[^"]*"[^>]*>([^<]+)</div>',
            r'<div[^>]*class="[^"]*nav[^"]*"[^>]*>([^<]+)</div>'
        ]
        
        for pattern in top_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                link_matches = re.findall(r'<a[^>]*>([^<]+)</a>', match)
                for link_text in link_matches:
                    clean_text = re.sub(r'<[^>]+>', '', link_text).strip()
                    if clean_text and len(clean_text) > 2:
                        keywords.add(clean_text)
        
        return list(keywords)
    
    def _extract_title_keywords(self, html_content: str) -> List[str]:
        """æ¤œç´¢çµæœã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        # æ¤œç´¢çµæœã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
        title_pattern = r'<h3[^>]*>([^<]+)</h3>'
        titles = re.findall(title_pattern, html_content)
        
        for title in titles:
            clean_title = re.sub(r'<[^>]+>', '', title).strip()
            if clean_title:
                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦ãªå˜èªã‚’æŠ½å‡º
                words = re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+', clean_title)
                for word in words:
                    if len(word) > 1:
                        keywords.add(word)
        
        return list(keywords)
    
    def _extract_description_keywords(self, html_content: str) -> List[str]:
        """æ¤œç´¢çµæœã®èª¬æ˜æ–‡ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        # æ¤œç´¢çµæœã®èª¬æ˜æ–‡ã‚’æŠ½å‡º
        desc_patterns = [
            r'<p[^>]*class="[^"]*description[^"]*"[^>]*>([^<]+)</p>',
            r'<div[^>]*class="[^"]*description[^"]*"[^>]*>([^<]+)</div>',
            r'<span[^>]*class="[^"]*description[^"]*"[^>]*>([^<]+)</span>'
        ]
        
        for pattern in desc_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                clean_text = re.sub(r'<[^>]+>', '', match).strip()
                if clean_text:
                    # èª¬æ˜æ–‡ã‹ã‚‰é‡è¦ãªå˜èªã‚’æŠ½å‡º
                    words = re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+', clean_text)
                    for word in words:
                        if len(word) > 1 and len(word) < 15:
                            keywords.add(word)
        
        return list(keywords)
    
    def _extract_url_keywords(self, html_content: str) -> List[str]:
        """æ¤œç´¢çµæœã®URLã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        keywords = set()
        
        # æ¤œç´¢çµæœã®URLã‚’æŠ½å‡º
        url_pattern = r'<a[^>]*href="([^"]*)"[^>]*>([^<]+)</a>'
        url_matches = re.findall(url_pattern, html_content)
        
        for url, link_text in url_matches:
            # URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’æŠ½å‡º
            if 'yahoo.co.jp' not in url and 'google.com' not in url:
                # å¤–éƒ¨ã‚µã‚¤ãƒˆã®URLã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                domain_match = re.search(r'https?://(?:www\.)?([^/]+)', url)
                if domain_match:
                    domain = domain_match.group(1)
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³åã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                    domain_words = re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+', domain)
                    for word in domain_words:
                        if len(word) > 1:
                            keywords.add(word)
            
            # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            clean_text = re.sub(r'<[^>]+>', '', link_text).strip()
            if clean_text:
                words = re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+', clean_text)
                for word in words:
                    if len(word) > 1:
                        keywords.add(word)
        
        return list(keywords)
    
    def _make_safe_filename(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›"""
        safe_text = re.sub(r'[<>:"/\\|?*]', '_', text)
        safe_text = re.sub(r'\s+', '_', safe_text)
        safe_text = safe_text[:100]
        return safe_text
    
    def clear_cache(self, older_than_hours: int = 24):
        """å¤ã„HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        current_time = time.time()
        cutoff_time = current_time - (older_than_hours * 3600)
        
        deleted_count = 0
        for file_path in self.output_dir.glob("*.html"):
            if file_path.stat().st_mtime < cutoff_time:
                file_path.unlink()
                deleted_count += 1
        
        if deleted_count > 0:
            print(f"[INFO] {deleted_count}ä»¶ã®å¤ã„HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")

# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
async def test_100_keyword_collector():
    """é«˜é€Ÿ100å€‹ç‰ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åé›†ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== Yahooæ¤œç´¢ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åé›†ãƒ†ã‚¹ãƒˆï¼ˆé«˜é€Ÿ100å€‹ç‰ˆï¼‰ ===")
    
    collector = YahooKeywordCollector100()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    test_keywords = [
        "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å­¦ç¿’",
        "æ–™ç† ä½œã‚Šæ–¹"
    ]
    
    for keyword in test_keywords:
        print(f"\n{'='*50}")
        print(f"ãƒ†ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")
        print(f"{'='*50}")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åé›†ã‚’å®Ÿè¡Œ
        start_time = time.time()
        collected_keywords = await collector.collect_all_keywords(keyword)
        elapsed_time = time.time() - start_time
        
        print(f"\nåé›†ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸Šä½20ä»¶ï¼‰:")
        for i, kw in enumerate(collected_keywords[:20], 1):
            print(f"  {i:2d}. {kw}")
        
        if len(collected_keywords) > 20:
            print(f"  ... ä»– {len(collected_keywords) - 20}ä»¶")
        
        print(f"\nåˆè¨ˆ: {len(collected_keywords)}ä»¶")
        print(f"å‡¦ç†æ™‚é–“: {elapsed_time:.1f}ç§’")
        
        # 100å€‹ç›®æ¨™ã®é”æˆçŠ¶æ³
        if len(collected_keywords) >= 100:
            print(f"ğŸ¯ ç›®æ¨™é”æˆï¼ 100å€‹ä»¥ä¸Šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åé›†ã—ã¾ã—ãŸã€‚")
        else:
            print(f"ğŸ“Š ç›®æ¨™ã¾ã§ã‚ã¨ {100 - len(collected_keywords)}å€‹")
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿
        await asyncio.sleep(1)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    collector.clear_cache()

if __name__ == "__main__":
    asyncio.run(test_100_keyword_collector())
